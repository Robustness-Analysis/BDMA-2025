# Perform hierarchical clustering
hclusters <- hclust(distance_matrix, method = "ward")
# Plot dendrogram with labels
plot(hclusters, hang = -1, labels = wide_data$technique, main = "Hierarchical Grouping", xlab = "Observations", sub = NULL)
rect.hclust(hclusters, k = i, border = "red")
# Plot dendrogram without labels
plot(hclusters, hang = -1, main = "Hierarchical Grouping", xlab = "Observations", sub = NULL)
rect.hclust(hclusters, k = i, border = "red")
# Divide dendrogram into groups
groups <- cutree(hclusters, k = i)
# Compute silhouette coefficient for hierarchical clustering
sil <- silhouette(groups, dist = distance_matrix)
# Create silhouette plot
plot(sil)
}
install.packages("RWeka")
library(RWeka)
# Packages that need to be loaded
pacman::p_load(caret, iml, xtable, ggpubr, citation, dplyr, earth, lime)
version
knitr::opts_chunk$set(echo = TRUE)
# Packages that need to be loaded
library(dplyr)
library(tidyr)
library(ggplot2)
library(GGally) # extensiÃ³n de ggplot2
library(factoextra) # visualizacion de los clusters
library(NbClust) # determinar el mejor numero de grupos
library(cluster) # medidas de evaluacion como silhouette
library(flexclust)
library(smacof)
library(MASS)
# Load files
datasets <- readRDS("../files/datasets.rds")
method_names = readRDS("../files/method_names.rds")
noise_level <- readRDS("../files/noise.rds")
noise_names <- readRDS("../files/noise_names.rds")
instances_names = readRDS("../files/instances_names.rds")
quartiles_names = c("25", "50", "75", "100")
# Load results
deciles_df <- readRDS("../results/KLC_plot_deciles.rds")
#deciles_df <- deciles_df %>% select(-accuracy, -kappa, -dataset_order, -method_order)
saveRDS(deciles_df, file = "../results/KLC_plot_deciles.rds")
quartiles_df <- readRDS("../results/KLC_plot_quartiles.rds")
#quartiles_df <- quartiles_df %>% select(-accuracy, -kappa, -dataset_order, -method_order)
saveRDS(quartiles_df, file = "../results/KLC_plot_quartiles.rds")
# Function to get the number of instances from a dataset
get_num_instances <- function(dataset) {
num_instances <- nrow(dataset)
return(num_instances)
}
# Function to determine the type of each attribute
get_attribute_type <- function(dataset) {
# Exclude the "class" column
dataset_subset <- dataset[, !names(dataset) %in% "class"]
# Initialize counts
numerical_count <- 0
nominal_count <- 0
# Loop through columns
for (col in names(dataset_subset)) {
if (is.numeric(dataset_subset[[col]])) {
numerical_count <- numerical_count + 1
} else {
nominal_count <- nominal_count + 1
}
}
# Return counts
return(list(numerical = numerical_count, nominal = nominal_count))
}
# Function to determine if dataset is binary or multiclass
is_binary <- function(dataset) {
unique_classes <- unique(dataset$class)
num_unique_classes <- length(unique_classes)
if (num_unique_classes == 2) {
return("binary")
} else if (num_unique_classes > 2) {
return("multiclass")
} else {
stop("Invalid number of unique classes")
}
}
# Create a dataframe to store the summary
characteristics_df <- data.frame(
dataset_name = character(0),
instances_n = numeric(0),
attributes_num = numeric(0),
attributes_nom = numeric(0),
dataset_type = character(0),
stringsAsFactors = FALSE
)
# Iterate through datasets
for (dataset_name in datasets) {
# Load dataset
filename = paste0("../datasets/", dataset_name, ".rds")
dataset <- readRDS(filename)
# Calculate summary metrics
num_instances <- get_num_instances(dataset)
attribute_types <- get_attribute_type(dataset)
dataset_type <- is_binary(dataset)
# Add to summary dataframe
characteristics_df <- bind_rows(characteristics_df, data.frame(
dataset_name = dataset_name,
instances_n = num_instances,
attributes_num = attribute_types$numerical,
attributes_nom = attribute_types$nominal,
dataset_type = dataset_type
))
}
characteristics_df$dataset_type = ifelse(characteristics_df$dataset_type == "binary", 0, 1)
# Convert all data to numeric
characteristics_df$instances_n <- as.numeric(characteristics_df$instances_n)
characteristics_df$attributes_num <- as.numeric(characteristics_df$attributes_num)
characteristics_df$attributes_nom <- as.numeric(characteristics_df$attributes_nom)
characteristics_df$dataset_type <- as.numeric(characteristics_df$dataset_type)
# Combine attribute columns
#characteristics_df$attributes <- characteristics_df$attributes_num + characteristics_df$attributes_nom
# Print the summary dataframe
print(characteristics_df)
saveRDS(characteristics_df, "../files/clustering/characteristics.rds")
# Create a vector of colors based on the number of clusters
colors <- c("#F7AC36","#BF1F5A", "#A1DF91", "#3151CC", "#FF2F20", "#A44FB2")
# Create a named vector to map clusters to specific colors
cluster_colors <- c(
"1" = "#F7AC36",
"2" = "#BF1F5A",
"3" = "#A1DF91",
"4" = "#3151CC",
"5" = "#FF2F20"
) #F7AC36, #BF1F5A
# Euclidean distances between the rows
clusters_scaled <- scale(characteristics_df[,-1])
# Add dataset names back
clusters_scaled <- cbind(dataset_name = characteristics_df$dataset_name, clusters_scaled)
# Turn into dataframe
clusters_scaled <- as.data.frame(clusters_scaled)
# Print the scaled dataframe
print(clusters_scaled)
saveRDS(clusters_scaled, "../files/clustering/cl_scaled.rds")
clusters_scaled$instances_n <- as.numeric(clusters_scaled$instances_n)
clusters_scaled$attributes_num <- as.numeric(clusters_scaled$attributes_num)
clusters_scaled$attributes_nom <- as.numeric(clusters_scaled$attributes_nom)
clusters_scaled$dataset_type <- as.numeric(clusters_scaled$dataset_type)
saveRDS(clusters_scaled, "../files/clustering/cl_scaled.rds")
set.seed(1)
# Euclidean distances between the rows
dmatrix <- dist(clusters_scaled[,-1])
# Print the distance matrix
print(dmatrix)
saveRDS(dmatrix, "../files/clustering/dmatrix.rds")
# "silhouette", "wss", "gap_stat"
print(fviz_nbclust(clusters_scaled[,-1], kmeans, method = "wss") + geom_vline(xintercept = 4, linetype = 2) + labs(subtitle = "Elbow method") + theme_minimal())
# Create a new table with dataset names and rownames
groups_df <- cbind(ID = rownames(clusters_scaled), dataset_name = clusters_scaled$dataset_name)
# Turn into dataframe
groups_df <- as.data.frame(groups_df)
# Make sure all necessary data is numeric
groups_df$ID <- as.numeric(groups_df$ID)
# Perform hierarchical clustering
hclusters <- hclust(dmatrix, method = "ward.D")
# Plot dendrogram
plot(hclusters, labels = rownames(clusters_scaled), main = "Heirarchal Grouping", xlab = "Observations", sub = NULL, ylim=c(0,1))
# Divide dendogram into groups
k_list <- c(3, 4, 5, 6)
for(i in k_list) {
groups <- cutree(hclusters, k = i)
rect.hclust(hclusters, k = i, border = colors[1:i])
# Add dendogram groups to the data
# Create column name dynamically using paste0
column_name <- paste0("hclust", i)
groups_df[[column_name]] <- groups
# Compute silhouette coefficient for hierarchical clustering
sil <- silhouette(groups, dmatrix)
# Create silhouette plot
plot(sil, col = colors[1:i], main = paste0("HClust Silhouette with K = ", i))
}
# Divide dendogram into groups
# k_list <- c(3, 4, 5, 6)
k_list <- c(3, 4, 5, 6)
for(i in k_list) {
# Set seed for reproducibility
set.seed(1)
# Run kmeans clustering with dynamic number of centers
kmeans <- kmeans(clusters_scaled[,-1], centers = i, nstart = 25)
# Extract the centroids
centroids <- kmeans$centers
# Convert result of kmeans to kcca from flexclust
kcca <- as.kcca(kmeans, clusters_scaled[,-1])
# Add cluster labels to the groups dataframe with dynamic name
column_name <- paste0("kmeans_", i)
groups_df[[column_name]] <- kmeans$cluster
# Visualize the clusters with custom colors
cluster_plot <- fviz_cluster(kmeans, data = clusters_scaled[,-1],
geom = "point", ellipse.type = "convex",
ggtheme = theme_minimal())
# Apply the custom color palette
cluster_plot <- cluster_plot +
scale_color_manual(values = colors[1:i]) +
scale_fill_manual(values = colors[1:i]) +
coord_fixed(ratio = 1)
# Display the plot
print(cluster_plot)
# Compute silhouette coefficient for kmeans clustering
sil_result <- silhouette(kmeans$cluster, dmatrix, clusters_scaled$dataset_name)
# Create silhouette plot
plot(sil_result, col = colors[1:i], main = paste0("K = ", i, " Silhouette"))
}
# Save the kmeans for k = 4
saveRDS(kmeans, file = "../files/clustering/kmeans.rds")
# Save the centroids
saveRDS(centroids, file = "../files/clustering/centroids.rds")
# Save the KCCA
saveRDS(kcca, file = "../files/clustering/kcca.rds")
# Compute Shepard for the distance matrix
fit_mds <- mds(dmatrix)
plot(fit_mds, plot.type = "Shepard")
# Nonmetric MDS
# N rows (objects) x p columns (variables)
# each row identified by a unique row name
fit <- isoMDS(dmatrix, k=2) # k is the number of dim
# Plot solution without labels
x <- fit$points[,1]
y <- fit$points[,2]
plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2",
main="Kruskal's Non-metric MDS", type="n")
points(x, y)
# Create a table with the results
coord_df <- data.frame(
ID = clusters_scaled$dataset_name,
Coordinate1 = x,
Coordinate2 = y
)
names(coord_df)[names(coord_df) == 'ID'] <- 'dataset_name'
names(coord_df)[names(coord_df) == 'Coordinate1'] <- 'coordinate_1'
names(coord_df)[names(coord_df) == 'Coordinate2'] <- 'coordinate_2'
# Print the results table
print(coord_df)
# Merge the dataframes
groups_df <- left_join(groups_df, coord_df, by = "dataset_name")
# Plot solution
x <- groups_df$coordinate_1
y <- groups_df$coordinate_2
# K = 4
# Create the plot
plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2",
main="Kruskal's Non-metric MDS", type="n")
# Define colors for each cluster
point_colors <- colors[groups_df$kmeans_4]
# Add points colored by cluster
points(x, y, col=point_colors, pch=16)
# Add text labels
#text(x, y, labels=plot_df[,1], cex=0.7, col=point_colors)
# Plot solution
x <- groups_df$coordinate_1
y <- groups_df$coordinate_2
# K = 5
# Create the plot
plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2",
main="Kruskal's Non-metric MDS", type="n")
# Define colors for each cluster
point_colors <- colors[groups_df$kmeans_5]
# Add points colored by cluster
points(x, y, col=point_colors, pch=16)
# Add text labels
#text(x, y, labels=plot_df[,1], cex=0.7, col=point_colors)
# Plot solution
x <- groups_df$coordinate_1
y <- groups_df$coordinate_2
# K = 5
# Create the plot
plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2",
main="Kruskal's Non-metric MDS", type="n")
point_colors <- colors[groups_df$kmeans_5]
# Add points colored by cluster
points(x, y, col=point_colors, pch=16)
# Add text labels
#text(x, y, labels=plot_df[,1], cex=0.7, col=point_colors)
# Save the groups
saveRDS(groups_df, "../files/clustering/groups.rds")
df3 <- deciles_df %>%
group_by(dataset_name, noise, percentage) %>%
summarize(kappa_loss = round(mean(kappa_loss, na.rm = TRUE), 2)) %>%
ungroup()
df3_q <- quartiles_df %>%
group_by(dataset_name, noise, percentage) %>%
summarize(kappa_loss = round(mean(kappa_loss, na.rm = TRUE), 2)) %>%
ungroup()
#saveRDS(df3, file = "../results/meanKLC_d2.rds")
#saveRDS(df3_q, file = "../results/meanKLC_q2.rds")
# K = 5 is optimal
df3_q_with_clusters <- df3_q %>%
left_join(groups_df[, c("dataset_name", "kmeans_5")], by = "dataset_name") %>%
rename(cluster = kmeans_5)
# Get the dataset KLC results with cluster information
cluster_results <- df3_q_with_clusters[c("cluster", "dataset_name", "noise", "percentage", "kappa_loss")]
print(cluster_results)
cluster_means <- cluster_results %>%
group_by(cluster, noise, percentage) %>%
summarize(kappa_loss = round(mean(kappa_loss, na.rm = TRUE), 2)) %>%
ungroup()
print(cluster_means)
# Save the results
#saveRDS(cluster_means, file = "../results/cluster_means.rds")
for(instance in quartiles_names) {
# Filter data for the current instance percentage
filtered_data <- subset(df3_q, percentage == instance)
# Join with cluster information
filtered_data_with_clusters <- filtered_data %>%
left_join(groups_df[, c("dataset_name", "kmeans_5")], by = "dataset_name") %>%
rename(cluster = kmeans_5)
# Create plot with colors based on clusters
p1 <- ggplot(filtered_data_with_clusters, aes(x = noise, y = kappa_loss,
color = factor(cluster),
group = dataset_name)) +
geom_point() +
geom_line() +
scale_color_manual(values = cluster_colors, name = "Cluster") +
labs(x = "Noise", y = "Kappa Loss") +
ggtitle(paste0("Kappa Loss Curves by dataset (colored by cluster), noise and ",
instance, " % of instances altered")) +
theme_bw() +
scale_y_continuous(limits = c(0.0, 0.8), breaks = seq(0, 1, by = 0.1))
# Print plot
print(p1)
}
for(instance in quartiles_names) {
# Filter data for the current instance percentage
filtered_data <- subset(cluster_means, percentage == instance)
# Create plot with consistent cluster colors
p2 <- ggplot(filtered_data, aes(x = noise, y = kappa_loss, color = factor(cluster))) +
geom_point() +
geom_line() +
scale_color_manual(values = cluster_colors, name = "Cluster") +
labs(x = "Noise", y = "Kappa Loss") +
ggtitle(paste0("Kappa Loss Curves by cluster, noise and ",
instance, " % of instances altered")) +
theme_bw() +
scale_y_continuous(limits = c(0.0, 0.5), breaks = seq(0, 1, by = 0.1))
# Print plot
print(p2)
}
plot_list <- list()
# Create all plots and store them in the list
for(i in seq_along(quartiles_names)) {
instance <- quartiles_names[i]
# Filter data for both individual datasets and clusters
filtered_dataset_data <- subset(df3_q_with_clusters, percentage == instance)
filtered_cluster_data <- subset(cluster_means, percentage == instance)
# Create combined plot
combined_dataset_plot <- ggplot() +
# Add individual dataset lines with colors based on their cluster
geom_line(data = filtered_dataset_data,
aes(x = noise, y = kappa_loss, group = dataset_name, color = factor(cluster)),
linetype = "solid", alpha = 0.5) +
geom_point(data = filtered_dataset_data,
aes(x = noise, y = kappa_loss, group = dataset_name, color = factor(cluster)),
alpha = 0.5) +
# Add thicker cluster lines to show the averages
geom_line(data = filtered_cluster_data,
aes(x = noise, y = kappa_loss, group = cluster, color = factor(cluster)),
linewidth = 1.5) +
geom_point(data = filtered_cluster_data,
aes(x = noise, y = kappa_loss, group = cluster, color = factor(cluster)),
size = 3) +
# Set the specific color mapping
scale_color_manual(name = "Cluster", values = cluster_colors) +
# Customize the plot
scale_y_continuous(limits = c(0.0, 0.8), breaks = seq(0, 1, by = 0.1)) +
labs(x = "Noise",
y = "Kappa Loss",
title = paste0(instance, "% of instances altered")) +
theme_bw() +
theme(legend.position = "right")
# Store plot in list
plot_list[[i]] <- combined_dataset_plot
}
# Arrange all plots in a grid using patchwork or gridExtra
if (requireNamespace("patchwork", quietly = TRUE)) {
# Using patchwork (preferred if available)
library(patchwork)
dataset_combined_grid <- wrap_plots(plot_list, ncol = 2) +
plot_annotation(title = "Kappa Loss Curves by Dataset and Cluster")
print(dataset_combined_grid)
} else if (requireNamespace("gridExtra", quietly = TRUE)) {
# Using gridExtra as an alternative
library(gridExtra)
dataset_grid_title <- textGrob("Kappa Loss Curves by Dataset and Cluster", gp = gpar(fontsize = 14))
dataset_grid_arranged <- grid.arrange(grobs = plot_list, ncol = 2, top = dataset_grid_title)
print(dataset_grid_arranged)
} else {
# If neither package is available, print a message and show plots individually
message("Please install either 'patchwork' or 'gridExtra' package for grid layout.")
for (p in plot_list) {
print(p)
}
}
# Save the grid plot
ggsave("../results/plots/cluster_curves_grid2.png", dataset_combined_grid, width = 40, height = 40, dpi = 600)
knitr::opts_chunk$set(echo = TRUE)
# Load results
mia_df <- readRDS("../results/most_important_attr/mia_df.rds")
View(mia_df)
setwd("~/github/BDMA-2025")
# Packages that need to be loaded
library(dplyr)
library(ggplot2)
# Load files
datasets <- readRDS("../files/datasets.rds")
method_names = readRDS("../files/method_names.rds")
noise_level <- readRDS("../files/noise.rds")
noise_names <- readRDS("../files/noise_names.rds")
instances_names = readRDS("../files/instances_names.rds")
quartiles_names = c("0", "25", "50", "75", "100")
# Load results
mia_df <- readRDS("../results/most_important_attr/mia_df.rds")
noiseMIA_list <- readRDS("../results/noise/noise_list.rds")
#instancesCM_list = readRDS("../results/instances/instancesCM_list.rds")
#confusion_list <- readRDS("../results/conf_matrices/confusion_matrices.rds")
instancesCM_list = readRDS("../results/instances/instancesCM_list_popular.rds")
confusion_list <- readRDS("../results/conf_matrices/confusion_matrices_popular.rds")
# Function to get the number of instances from a dataset
get_num_instances <- function(dataset) {
num_instances <- nrow(dataset)
return(num_instances)
}
# Function to determine the type of each attribute
get_attribute_type <- function(dataset) {
# Exclude the "class" column
dataset_subset <- dataset[, !names(dataset) %in% "class"]
# Initialize counts
numerical_count <- 0
nominal_count <- 0
# Loop through columns
for (col in names(dataset_subset)) {
if (is.numeric(dataset_subset[[col]])) {
numerical_count <- numerical_count + 1
} else {
nominal_count <- nominal_count + 1
}
}
# Return counts
return(list(Numerical = numerical_count, Nominal = nominal_count))
}
# Function to determine if dataset is binary or multiclass
is_binary <- function(dataset) {
unique_classes <- unique(dataset$class)
num_unique_classes <- length(unique_classes)
if (num_unique_classes == 2) {
return("Binary")
} else if (num_unique_classes > 2) {
return("Multiclass")
} else {
stop("Invalid number of unique classes")
}
}
# Create a dataframe to store the summary
summary_df <- data.frame(
Dataset = character(0),
Num_Instances = numeric(0),
Numerical_Attributes = numeric(0),
Nominal_Attributes = numeric(0),
Dataset_Type = character(0),
stringsAsFactors = FALSE
)
# Iterate through datasets
for (dataset_name in datasets) {
# Load dataset
filename = paste0("../datasets/", dataset_name, ".rds")
dataset <- readRDS(filename)
# Calculate summary metrics
num_instances <- get_num_instances(dataset)
attribute_types <- get_attribute_type(dataset)
dataset_type <- is_binary(dataset)
# Add to summary dataframe
summary_df <- bind_rows(summary_df, data.frame(
Dataset = dataset_name,
Num_Instances = num_instances,
Numerical_Attributes = attribute_types$Numerical,
Nominal_Attributes = attribute_types$Nominal,
Dataset_Type = dataset_type
))
}
# Convert all data to numeric
summary_df$Num_Instances <- as.numeric(summary_df$Num_Instances)
summary_df$Numerical_Attributes <- as.numeric(summary_df$Numerical_Attributes)
summary_df$Nominal_Attributes <- as.numeric(summary_df$Nominal_Attributes)
# Combine attribute columns
#summary_df$Attributes <- summary_df$Numerical_Attributes + summary_df$Nominal_Attributes
# Print the summary dataframe
print(summary_df)
# Turn list into dataframe
deciles_df = data.frame(matrix(ncol = 6, nrow = 0))
colnames(deciles_df) = c("dataset_name", "technique", "noise", "percentage", "accuracy", "kappa")
for(dataset in datasets) {
for(method in method_names) {
for(noise in noise_names) {
for(instance in instances_names){
# Get the values for accuracy and kappa
#a <- confusion_list[[dataset]][[1]][[method]][[noise]][[instance]]$accuracy
a <- confusion_list[[dataset]][[1]][[method]][[noise]][[instance]]$accuracy
#k <- confusion_list[[dataset]][[1]][[method]][[noise]][[instance]]$kappa
k <- confusion_list[[dataset]][[1]][[method]][[noise]][[instance]]$kappa
# Add row to results dataframe
deciles_df[nrow(deciles_df) + 1,] = c(dataset, method, noise, instance, a, k)
}
}
}
}
View(confusion_list)
a <- confusion_list[[dataset]]
View(a)
a <- confusion_list[[dataset]][[1]]
a <- confusion_list[[dataset]][[1]][[method]]
