"1" = "#4FB28F",  # Green
"2" = "#8F4FB2",  # Purple
"3" = "#3681F7",  # Blue
"4" = "#F65215"   # Orange
)
# Save the dendrogram with colored rectangles by cluster
png("../results/plots/dendogram.png", width = 4000, height = 3000, res = 600)
plot(hclusters, hang = -1, labels = wide_data$technique,
main = paste("Hierarchical Grouping (k =", k, ")"),
xlab = "Observations", sub = NULL)
# Create colored rectangles with consistent colors per cluster
rect.hclust(hclusters, k = k, border = cluster_colors[as.character(1:k)])
dev.off()
# Generate silhouette plot with consistent colors
png("../results/plots/silhouette_t.png", width = 4000, height = 3000, res = 600)
sil <- silhouette(clusters, dist = distance_matrix)
# Use the same colors for silhouette plot as for dendrogram
plot(sil, col = cluster_colors[as.character(sort(unique(clusters)))],
main = paste("Silhouette Plot (k =", k, ")"))
dev.off()
# Join cluster assignments with original data
meanKLC_q_with_clusters <- meanKLC_q %>%
left_join(technique_clusters, by = "technique")
# Calculate mean kappa loss for each cluster, noise level, and percentage
cluster_means <- meanKLC_q_with_clusters %>%
group_by(cluster, noise, percentage) %>%
summarize(kappa_loss = round(mean(kappa_loss, na.rm = TRUE), 2), .groups = 'drop')
print(cluster_means)
# Create plots for individual techniques (optional)
for(instance in quartiles_names) {
# Filter data for the current instance percentage
filtered_data <- subset(meanKLC_q_with_clusters, percentage == instance)
# Create plot with consistent colors
p1 <- ggplot(filtered_data, aes(x = noise, y = kappa_loss, color = factor(cluster))) +
geom_point() +
geom_line(aes(group = technique)) +
# Use consistent colors based on cluster assignment
scale_color_manual(values = cluster_colors) +
labs(x = "Noise", y = "Kappa Loss", color = "Cluster") +
ggtitle(paste0("Kappa Loss Curves by technique, noise and ", instance, " % of instances altered")) +
theme_bw() +
scale_y_continuous(limits = c(0.0, 0.5), breaks = seq(0, 1, by = 0.1))
# Print plot
print(p1)
}
knitr::opts_chunk$set(echo = TRUE)
# Packages that need to be loaded
library(dplyr)
library(tidyr)
library(ggplot2)
library(GGally) # extensiÃ³n de ggplot2
library(factoextra) # visualizacion de los clusters
library(NbClust) # determinar el mejor numero de grupos
library(cluster) # medidas de evaluacion como silhouette
# Load files
datasets <- readRDS("../files/datasets.rds")
method_names = readRDS("../files/method_names.rds")
noise_level <- readRDS("../files/noise.rds")
noise_names <- readRDS("../files/noise_names.rds")
instances_names = readRDS("../files/instances_names.rds")
quartiles_names = c("25", "50", "75", "100")
# Load results
meanKLC <- readRDS("../results/meanKLC_d.rds") # This is df2 from Aggregate_Curves
meanKLC_q <- readRDS("../results/meanKLC_q.rds") # This is df2_q from Aggregate_Curves
# Transform the data
wide_data <- meanKLC_q %>%
unite("noise_percentage", noise, percentage, sep = "_") %>%
spread(key = noise_percentage, value = kappa_loss)
# View the transformed data
print(wide_data)
distance_matrix <- dist(wide_data, method = "euclidean")
# Define consistent color palette for clusters
cluster_colors <- c(
"1" = "#4FB28F",  # Green
"2" = "#8F4FB2",  # Purple
"3" = "#3681F7",  # Blue
"4" = "#F65215"   # Orange
)
# Perform hierarchical clustering
hclusters <- hclust(distance_matrix, method = "ward.D")
# Cut the tree to get k=4 clusters
k <- 4
clusters <- cutree(hclusters, k = k)
# Print cluster assignments
print(clusters)
# First get unique techniques in the same order as used for clustering
techniques <- wide_data$technique
# Create the mapping dataframe
technique_clusters <- data.frame(
technique = techniques,
cluster = clusters
)
# Create a named vector to map colors to specific clusters
# This ensures consistent color usage across all plots
cluster_colors <- c(
"1" = "#4FB28F",  # Green
"2" = "#8F4FB2",  # Purple
"3" = "#3681F7",  # Blue
"4" = "#F65215"   # Orange
)
# Save the dendrogram with colored rectangles by cluster
png("../results/plots/dendogram.png", width = 4000, height = 3000, res = 600)
plot(hclusters, hang = -1, labels = wide_data$technique,
main = paste("Hierarchical Grouping (k =", k, ")"),
xlab = "Observations", sub = NULL)
# Create colored rectangles with consistent colors per cluster
rect.hclust(hclusters, k = k, border = cluster_colors[as.character(1:k)])
dev.off()
# Generate silhouette plot with consistent colors
png("../results/plots/silhouette_t.png", width = 4000, height = 3000, res = 600)
sil <- silhouette(clusters, dist = distance_matrix)
# Use the same colors for silhouette plot as for dendrogram
plot(sil, col = cluster_colors[as.character(sort(unique(clusters)))],
main = paste("Silhouette Plot (k =", k, ")"))
dev.off()
# Join cluster assignments with original data
meanKLC_q_with_clusters <- meanKLC_q %>%
left_join(technique_clusters, by = "technique")
# Calculate mean kappa loss for each cluster, noise level, and percentage
cluster_means <- meanKLC_q_with_clusters %>%
group_by(cluster, noise, percentage) %>%
summarize(kappa_loss = round(mean(kappa_loss, na.rm = TRUE), 2), .groups = 'drop')
print(cluster_means)
# Create plots for individual techniques (optional)
for(instance in quartiles_names) {
# Filter data for the current instance percentage
filtered_data <- subset(meanKLC_q_with_clusters, percentage == instance)
# Create plot with consistent colors
p1 <- ggplot(filtered_data, aes(x = noise, y = kappa_loss, color = factor(cluster))) +
geom_point() +
geom_line(aes(group = technique)) +
# Use consistent colors based on cluster assignment
scale_color_manual(values = cluster_colors) +
labs(x = "Noise", y = "Kappa Loss", color = "Cluster") +
ggtitle(paste0("Kappa Loss Curves by technique, noise and ", instance, " % of instances altered")) +
theme_bw() +
scale_y_continuous(limits = c(0.0, 0.5), breaks = seq(0, 1, by = 0.1))
# Print plot
print(p1)
}
# Create plots for cluster means
for(instance in quartiles_names) {
# Filter data for the current instance percentage
filtered_data <- subset(cluster_means, percentage == instance)
# Create plot with consistent colors
p2 <- ggplot(filtered_data, aes(x = noise, y = kappa_loss, color = factor(cluster))) +
geom_point() +
geom_line(aes(group = cluster)) +
# Use consistent colors based on cluster assignment
scale_color_manual(values = cluster_colors) +
labs(x = "Noise", y = "Kappa Loss", color = "Cluster") +
ggtitle(paste0("Kappa Loss Curves by cluster, noise and ", instance, " % of instances altered")) +
theme_bw() +
scale_y_continuous(limits = c(0.0, 0.5), breaks = seq(0, 1, by = 0.1))
# Print plot
print(p2)
}
# Create an empty list to store plots
plot_list <- list()
# Create all plots and store them in the list
for(i in seq_along(quartiles_names)) {
instance <- quartiles_names[i]
# Filter data for both techniques and clusters
filtered_tech_data <- subset(meanKLC_q_with_clusters, percentage == instance)
filtered_cluster_data <- subset(cluster_means, percentage == instance)
# Create combined plot with consistent colors
combined_plot <- ggplot() +
# Add technique lines with colors based on their cluster
geom_line(data = filtered_tech_data,
aes(x = noise, y = kappa_loss, group = technique, color = factor(cluster)),
linetype = "solid", alpha = 0.5) +
geom_point(data = filtered_tech_data,
aes(x = noise, y = kappa_loss, group = technique, color = factor(cluster)),
alpha = 0.5) +
# Add thicker cluster lines to show the averages
geom_line(data = filtered_cluster_data,
aes(x = noise, y = kappa_loss, group = cluster, color = factor(cluster)),
linewidth = 1.5) +
geom_point(data = filtered_cluster_data,
aes(x = noise, y = kappa_loss, group = cluster, color = factor(cluster)),
size = 3) +
# Set the specific color mapping - consistent with other plots
scale_color_manual(name = "Cluster", values = cluster_colors) +
# Customize the plot
scale_y_continuous(limits = c(0.0, 0.5), breaks = seq(0, 1, by = 0.1)) +
labs(x = "Noise",
y = "Kappa Loss",
title = paste0(instance, "% of instances altered")) +
theme_bw() +
theme(legend.position = "right")
# Store plot in list
plot_list[[i]] <- combined_plot
}
# Arrange all plots in a grid using patchwork
if (requireNamespace("patchwork", quietly = TRUE)) {
# Using patchwork
library(patchwork)
combined_grid <- wrap_plots(plot_list, ncol = 1) +
plot_annotation(title = "Kappa Loss Curves by Technique and Cluster")
print(combined_grid)
# Save the grid plot
png(filename = "../results/plots/cluster_curves_grid.png",
width = 4000, height = 12000, res = 600)
print(combined_grid)
dev.off()
} else {
# Print plots individually if patchwork is not available
for (p in plot_list) {
print(p)
}
}
knitr::opts_chunk$set(echo = TRUE)
# Libraries needed
library(dplyr)      # For data manipulation
library(caret)      # For createDataPartition and confusion matrix
library(ggplot2)    # For visualization
# Set colors
c_a = "#39a0ca"
c_b = "#478559"
c_c = "#161748"
c_d = "#f95d9b"
line = "#1d1e22"
# Set seed for reproducibility
set.seed(42)
# Total number of points
n <- 2000
# Calculate points per division based on proportions
n_a <- floor(n * 0.50)  # 50% of the data
n_b <- floor(n * 0.25)  # 25% of the data
n_c <- floor(n * 0.125)  # 25% of the data
n_d <- floor(n * 0.125)  # 25% of the data
# Based on the drawing:
# A is on the left side of the graph (x1 < threshold)
# B is on the bottom right quadrant
# C is on the top left of the right side
# D is on the top right of the right side
# Thresholds for dividing the space
x1_threshold <- 0.4  # Dividing left (A) from right (B,C,D)
x1_right_threshold <- 0.7  # Dividing C from D on the right side
x2_threshold <- 0.5  # Dividing B from C,D
# Division A: Left portion (50% of data)
a_x1 <- runif(n_a, 0, x1_threshold)
a_x2 <- runif(n_a, 0, 1)
a_labels <- rep("A", n_a)
# Division B: Bottom right portion (25% of data)
b_x1 <- runif(n_b, x1_threshold, 1)
b_x2 <- runif(n_b, 0, x2_threshold)
b_labels <- rep("B", n_b)
# Division C: Top left of right portion (12.5% of data)
c_x1 <- runif(n_c, x1_threshold, x1_right_threshold)
c_x2 <- runif(n_c, x2_threshold, 1)
c_labels <- rep("C", n_c)
# Division D: Top right of right portion (12.5% of data)
d_x1 <- runif(n_d, x1_right_threshold, 1)
d_x2 <- runif(n_d, x2_threshold, 1)
d_labels <- rep("D", n_d)
# Combine all data into a single dataframe
df <- data.frame(
x1 = c(a_x1, b_x1, c_x1, d_x1),
x2 = c(a_x2, b_x2, c_x2, d_x2),
division = c(a_labels, b_labels, c_labels, d_labels)
)
# Plot the data to visualize the divisions
p <- ggplot(df, aes(x = x1, y = x2, color = division)) +
geom_point(alpha = 0.8, show.legend = FALSE) +
scale_color_manual(values = c("A" = c_a, "B" = c_b, "C" = c_c, "D" = c_d)) +
labs(title = "Dataset with Four Divisions",
x = "x1", y = "x2") +
theme_minimal() +
theme(,
axis.title.y = element_text(angle = 0, vjust = 0.5, hjust = 0.5)
)
print(p)
ggsave("original_data.png", plot = p, width = 40, height = 30, dpi = 600)
# Set training/test split proportion
train_prop <- 0.7 # 70% training, 30% test
# Create stratified split indices based on division
set.seed(42) # Keep the same seed for reproducibility
train_indices <- createDataPartition(df$division, p = train_prop, list = FALSE)
# Create training and test sets
train_df <- df[train_indices, ]
test_df <- df[-train_indices, ]
# Convert division to factor to ensure consistent levels
train_df$division <- factor(train_df$division)
test_df$division <- factor(test_df$division)
# Optional: Visualize the training and test sets
df$set <- "test"
df[train_indices, "set"] <- "train"
ggplot(df, aes(x = x1, y = x2, color = division, shape = set)) +
geom_point(alpha = 0.8) +
scale_color_manual(values = c("A" = c_a, "B" = c_b, "C" = c_c, "D" = c_d)) +
labs(title = "Training and Test Sets with Four Divisions",
x = "x1", y = "x2") +
theme_minimal() +
theme(,
axis.title.y = element_text(angle = 0, vjust = 0.5, hjust = 0.5)
)
# Load necessary libraries
library(caret)      # For confusion matrix
library(class)      # For kNN classification
library(e1071)      # For SVM
if (!require("nnet")) install.packages("nnet")
library(nnet)       # For multinomial logistic regression
if (!require("randomForest")) install.packages("randomForest")
library(randomForest) # For random forest
library(gridExtra)  # For arranging multiple plots
# Create a fine grid for decision boundary visualization
grid_size <- 100
x1_range <- seq(min(df$x1) - 0.1, max(df$x1) + 0.1, length.out = grid_size)
x2_range <- seq(min(df$x2) - 0.1, max(df$x2) + 0.1, length.out = grid_size)
grid <- expand.grid(x1 = x1_range, x2 = x2_range)
# Ensure column names match training data
colnames(grid) <- c("x1", "x2")  # Explicitly set column names
# Function to plot decision boundaries
plot_decision_boundary <- function(predictions, model_name, kappa_value) {
grid_predictions <- cbind(grid, pred = predictions)
# Plot decision boundaries with test data points
p <- ggplot() +
geom_raster(data = grid_predictions, aes(x = x1, y = x2, fill = pred), alpha = 0.8, show.legend = FALSE) +
#geom_point(data = test_df, aes(x = x1, y = x2, color = division), size = 2.5) +
scale_fill_manual(values = c("A" = c_a, "B" = c_b, "C" = c_c, "D" = c_d), name = "Predicted") +
scale_color_manual(values = c("A" = c_a, "B" = c_b, "C" = c_c, "D" = c_d), name = "Actual") +
#labs(x = "x1", y = "x2") +
scale_x_continuous(limits = c(0.0, 1), breaks = seq(0, 1, by = 0.25)) +
scale_y_continuous(limits = c(0.0, 1), breaks = seq(0, 1, by = 0.25)) +
labs(title = paste(model_name, "Decision Boundaries"),
subtitle = paste("Test Data with Cohen's Kappa =", round(kappa_value, 3)),
x = "x1", y = "x2") +
theme_minimal() +
theme(,
axis.title.y = element_text(angle = 0, vjust = 0.5, hjust = 0.5)
)
return(p)
}
evaluate_model <- function(actual, predicted, model_name) {
# Create confusion matrix
confusion <- confusionMatrix(predicted, actual)
# Extract Cohen's Kappa
kappa <- confusion$overall['Kappa']
# Print model performance
cat("\n", model_name, "Performance:\n")
cat("Cohen's Kappa:", kappa, "\n")
cat("Accuracy:", confusion$overall['Accuracy'], "\n\n")
return(kappa)
}
# Train KNN model (k=5)
k <- 5
knn_model <- knn(train = train_df[, c("x1", "x2")],
test = test_df[, c("x1", "x2")],
cl = train_df$division,
k = k)
# Evaluate KNN model
knn_kappa <- evaluate_model(test_df$division, knn_model, "KNN")
# Get KNN predictions for the grid
knn_grid_pred <- knn(train = train_df[, c("x1", "x2")],
test = grid,
cl = train_df$division,
k = k)
# Plot KNN decision boundaries
knn_plot <- plot_decision_boundary(knn_grid_pred, "K-Nearest Neighbors (k=5)", knn_kappa)
# Train multinomial model
multinom_model <- multinom(division ~ x1 + x2, data = train_df, trace = FALSE)
# Make predictions on test data
multinom_pred <- predict(multinom_model, newdata = test_df)
# Evaluate multinomial model
multinom_kappa <- evaluate_model(test_df$division, multinom_pred, "Multinomial Logistic Regression")
# Get multinomial predictions for the grid
multinom_grid_pred <- predict(multinom_model, newdata = grid)
# Plot multinomial decision boundaries
multinom_plot <- plot_decision_boundary(multinom_grid_pred, "Multinomial Logistic Regression", multinom_kappa)
# Train random forest model
rf_model <- randomForest(division ~ x1 + x2, data = train_df, ntree = 500)
# Make predictions on test data
rf_pred <- predict(rf_model, newdata = test_df)
# Evaluate random forest model
rf_kappa <- evaluate_model(test_df$division, rf_pred, "Random Forest")
# Get random forest predictions for the grid
rf_grid_pred <- predict(rf_model, newdata = grid)
# Plot random forest decision boundaries
rf_plot <- plot_decision_boundary(rf_grid_pred, "Random Forest", rf_kappa)
# Train SVM model with radial kernel
svm_model <- svm(division ~ x1 + x2, data = train_df, kernel = "radial")
# Make predictions on test data
svm_pred <- predict(svm_model, newdata = test_df)
# Evaluate SVM model
svm_kappa <- evaluate_model(test_df$division, svm_pred, "SVM")
# Get SVM predictions for the grid
svm_grid_pred <- predict(svm_model, newdata = grid)
# Plot SVM decision boundaries
svm_plot <- plot_decision_boundary(svm_grid_pred, "Support Vector Machine", svm_kappa)
# Display all plots
print(knn_plot)
print(multinom_plot)
print(rf_plot)
print(svm_plot)
ggsave("knn.png", plot = knn_plot, width = 20, height = 20, dpi = 600)
ggsave("multi.png", plot = knn_plot, width = 20, height = 20, dpi = 600)
ggsave("rf.png", plot = rf_plot, width = 20, height = 20, dpi = 600)
ggsave("svm.png", plot = svm_plot, width = 20, height = 20, dpi = 600)
# Standard deviation for the attributes
x1_sd <- sd(df$x1)
x2_sd <- sd(df$x2)
cat("Standard deviation of x1:", x1_sd, "\n")
cat("Standard deviation of x2:", x2_sd, "\n")
# Noise level (proportion of std dev to use)
noise_level <- 0.3  # You can adjust this between 0 and 1
set.seed(123)  # Different seed to ensure randomness
# Get test set indices
test_indices <- setdiff(1:nrow(df), train_indices)
# Randomly select half of the test set
half_test_size <- floor(length(test_indices) / 2)
noisy_test_indices <- sample(test_indices, half_test_size)
# Create first noisy dataset (noise in x1)
df_noisy1 <- df
# Add noise only to the selected half of test set
df_noisy1$x1[noisy_test_indices] <- df$x1[noisy_test_indices] +
rnorm(half_test_size, mean = 0, sd = x1_sd * noise_level)
# Create second noisy dataset (noise in x2)
df_noisy2 <- df
# Add noise only to the selected half of test set
df_noisy2$x2[noisy_test_indices] <- df$x2[noisy_test_indices] +
rnorm(half_test_size, mean = 0, sd = x2_sd * noise_level)
# Extract the test partition rows from the noisy datasets
noisytest_df1 <- df_noisy1[-train_indices, ]
noisytest_df2 <- df_noisy2[-train_indices, ]
# Ensure division is a factor with consistent levels in both datasets
noisytest_df1$division <- factor(noisytest_df1$division, levels = levels(test_df$division))
noisytest_df2$division <- factor(noisytest_df2$division, levels = levels(test_df$division))
# Create noisy grids for decision boundary visualization
# For noisy x1
noise_x1_range <- seq(min(df_noisy1$x1) - 0.1, max(df_noisy1$x1) + 0.1, length.out = grid_size)
grid_noisy1 <- expand.grid(x1 = noise_x1_range, x2 = x2_range)
# Ensure column names match training data
colnames(grid_noisy1) <- c("x1", "x2")  # Explicitly set column names
# For noisy x2
noise_x2_range <- seq(min(df_noisy2$x2) - 0.1, max(df_noisy2$x2) + 0.1, length.out = grid_size)
grid_noisy2 <- grid
# Ensure column names match training data
colnames(grid_noisy2) <- c("x1", "x2")  # Explicitly set column names
# Evaluate KNN model
knn_kappa <- evaluate_model(noisytest_df1$division, knn_model, "KNN")
# Get KNN predictions for the grid
knn_grid_pred <- knn(train = train_df[, c("x1", "x2")],
test = grid_noisy1,
cl = train_df$division,
k = k)
# Plot KNN decision boundaries
knn_plot1 <- plot_decision_boundary(knn_grid_pred, "K-Nearest Neighbors (k=5)", knn_kappa)
# Evaluate KNN model
knn_kappa <- evaluate_model(noisytest_df2$division, knn_model, "KNN")
# Get KNN predictions for the grid
knn_grid_pred <- knn(train = train_df[, c("x1", "x2")],
test = grid_noisy2,
cl = train_df$division,
k = k)
# Plot KNN decision boundaries
knn_plot2 <- plot_decision_boundary(knn_grid_pred, "K-Nearest Neighbors (k=5)", knn_kappa)
# Make predictions on test data
multinom_pred <- predict(multinom_model, newdata = noisytest_df1)
# Evaluate multinomial model
multinom_kappa <- evaluate_model(noisytest_df1$division, multinom_pred, "Multinomial Logistic Regression")
# Get multinomial predictions for the grid
multinom_grid_pred <- predict(multinom_model, newdata = grid_noisy1)
# Plot multinomial decision boundaries
multinom_plot1 <- plot_decision_boundary(multinom_grid_pred, "Multinomial Logistic Regression", multinom_kappa)
# Make predictions on test data
multinom_pred <- predict(multinom_model, newdata = noisytest_df2)
# Evaluate multinomial model
multinom_kappa <- evaluate_model(noisytest_df2$division, multinom_pred, "Multinomial Logistic Regression")
# Get multinomial predictions for the grid
multinom_grid_pred <- predict(multinom_model, newdata = grid_noisy2)
# Plot multinomial decision boundaries
multinom_plot2 <- plot_decision_boundary(multinom_grid_pred, "Multinomial Logistic Regression", multinom_kappa)
# Make predictions on test data
rf_pred <- predict(rf_model, newdata = noisytest_df1)
# Evaluate random forest model
rf_kappa <- evaluate_model(noisytest_df1$division, rf_pred, "Random Forest")
# Get random forest predictions for the grid
rf_grid_pred <- predict(rf_model, newdata = grid_noisy1)
# Plot random forest decision boundaries
rf_plot1 <- plot_decision_boundary(rf_grid_pred, "Random Forest", rf_kappa)
# Make predictions on test data
rf_pred <- predict(rf_model, newdata = noisytest_df2)
# Evaluate random forest model
rf_kappa <- evaluate_model(noisytest_df2$division, rf_pred, "Random Forest")
# Get random forest predictions for the grid
rf_grid_pred <- predict(rf_model, newdata = grid_noisy2)
# Plot random forest decision boundaries
rf_plot2 <- plot_decision_boundary(rf_grid_pred, "Random Forest", rf_kappa)
# Make predictions on test data
svm_pred <- predict(svm_model, newdata = noisytest_df1)
# Evaluate SVM model
svm_kappa <- evaluate_model(noisytest_df1$division, svm_pred, "SVM")
# Get SVM predictions for the grid
svm_grid_pred <- predict(svm_model, newdata = grid_noisy1)
# Plot SVM decision boundaries
svm_plot1 <- plot_decision_boundary(svm_grid_pred, "Support Vector Machine", svm_kappa)
# Make predictions on test data
svm_pred <- predict(svm_model, newdata = noisytest_df2)
# Evaluate SVM model
svm_kappa <- evaluate_model(noisytest_df2$division, svm_pred, "SVM")
# Get SVM predictions for the grid
svm_grid_pred <- predict(svm_model, newdata = grid_noisy2)
# Plot SVM decision boundaries
svm_plot2 <- plot_decision_boundary(svm_grid_pred, "Support Vector Machine", svm_kappa)
# Display all plots
print(knn_plot1)
print(multinom_plot1)
print(rf_plot1)
print(svm_plot1)
ggsave("knn_x1.png", plot = knn_plot1, width = 20, height = 20, dpi = 600)
setwd("~/github/BDMA-2025")
