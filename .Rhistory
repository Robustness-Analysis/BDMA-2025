confusion_matrix_2 <- instancesCM_list[[dataset]][["Fold_2"]][[method]][[noise]][[instance]]
confusion_matrix_3 <- instancesCM_list[[dataset]][["Fold_3"]][[method]][[noise]][[instance]]
confusion_matrix_4 <- instancesCM_list[[dataset]][["Fold_4"]][[method]][[noise]][[instance]]
confusion_matrix_5 <- instancesCM_list[[dataset]][["Fold_5"]][[method]][[noise]][[instance]]
# Store confusion matrices in a list
confusion_matrices <- list(confusion_matrix_1$table, confusion_matrix_2$table, confusion_matrix_3$table, confusion_matrix_4$table, confusion_matrix_5$table)
# Initialize a sum matrix with zeros
sum_matrix <- matrix(0, nrow = nrow(confusion_matrix_1$table), ncol = ncol(confusion_matrix_1$table))
# Add confusion matrices to the sum matrix element-wise
for (i in 1:length(confusion_matrices)) {
sum_matrix <- sum_matrix + confusion_matrices[[i]]
}
print("Total Sum Confusion Matrix:")
print(sum_matrix)
print(paste("Checking matrices for:", dataset, method, noise, instance))
print(paste0("CM1: ", !is.null(confusion_matrix_1) , " has table ", !is.null(confusion_matrix_1) && !is.null(confusion_matrix_1$table)))
# Calculate the average by dividing by the total number of confusion matrices
average_matrix <- sum_matrix / length(confusion_matrices)
if(noise == "noise_0") {
kappa <- 1
accuracy <- 1
} else if(instance == "0") {
kappa <- 1
accuracy <- 1
} else {
# Obtain average Accuracy and Kappa
# Compute the observed and expected agreements
total_obs <- sum(average_matrix)
P_o <- sum(diag(average_matrix)) / total_obs
P_e <- sum(rowSums(average_matrix) * colSums(average_matrix)) / (total_obs^2)
# Calculate Cohen's Kappa
kappa <- (P_o - P_e) / (1 - P_e)
# Calculate accuracy from a confusion matrix
accuracy <- sum(diag(average_matrix)) / sum(average_matrix)
if(kappa == "NaN") {kappa <- 1}
}
# Print relevant data
print(paste0("Technique: ", method))
print(paste0("Noise level: ", noise))
print(paste0("Instance percentage: ", instance))
# Print the average confusion matrix
print("Average Confusion Matrix:")
print(average_matrix)
# Print the average Cohen's Kappa
print("Average Cohen's Kappa:")
print(kappa)
# Print the accuracy
print("Accuracy:")
print(accuracy)
# Store elements in list
item_list[[1]] <- average_matrix
item_list[[2]] <- kappa
item_list[[3]] <- accuracy
names(item_list) <-  c("confusion_matrix", "kappa", "accuracy")
percentagesCM_list[[p_counter]] <- item_list
p_counter = p_counter + 1
print("---")
}
instancesCM_list[[dataset]][["Fold_1"]][[method]][[noise]]
# Obtain each confusion matrix from all five folds
confusion_matrix_1 <- instancesCM_list[[dataset]][["Fold_1"]][[method]][[noise]][[instance]]
confusion_matrix_1
instancesCM_list
clar
clear
instancesCM_list$vowel$Fold_1$C5.0$noise_20$`0`
instancesCM_list$vowel$Fold_1$C5.0$noise_20$`0`$table
instance
# /usr/bin/env/Rscript
setwd("~/rscripts/code")
# Packages that need to be loaded
#pacman::p_load(caret, iml, xtable, ggpubr, citation, dplyr, data.table)
# Set the seed to make the experiment reproducible
set.seed(1)
# Load important data
## Load files
args <- commandArgs(trailingOnly = TRUE)
datasets <- args
datasets <- readRDS("files/datasets.rds")
fold_names <- readRDS("files/folds.rds")
#method_names <- readRDS("files/method_names.rds")
method_names = c("C5.0", "ctree", "fda", "gbm", "gcvEarth", "JRip", "lvq", "mlpML", "multinom", "naive_bayes", "PART", "rbfDDA", "rda", "rf", "rpart", "simpls", "svmRadial", "rfRules", "knn", "bayesglm")
noise_names <- readRDS("files/noise_names.rds")
instances_names = readRDS("files/instances_names.rds")
#instances_names = append(readRDS("files/instances_names.rds"), c("25", "75"))
## Load results
mia_df <- readRDS("results/mia_df.rds")
noiseMIA_list <- readRDS("results/noise_list.rds")
instancesCM_list = readRDS("results/instancesCM_list_popular.rds")
confMatrices_list <- list()
cm_counter = 1
for(dataset in datasets) {
# Load dataset
dataset <- "vowel"
filename = paste0("datasets/", dataset, ".rds")
df <- readRDS(filename)
methodCM_list <- list()
m_counter = 1
for(method in method_names){
method = "C5.0"
noiseCM_list <- list()
n_counter = 1
for(noise in noise_names) {
noise = "noise_20"
percentagesCM_list <- list()
p_counter = 1
for(instance in instances_names){
instance = 0
item_list <- list()
# Obtain each confusion matrix from all five folds
confusion_matrix_1 <- instancesCM_list[[dataset]][["Fold_1"]][[method]][[noise]][[instance]]
confusion_matrix_2 <- instancesCM_list[[dataset]][["Fold_2"]][[method]][[noise]][[instance]]
confusion_matrix_3 <- instancesCM_list[[dataset]][["Fold_3"]][[method]][[noise]][[instance]]
confusion_matrix_4 <- instancesCM_list[[dataset]][["Fold_4"]][[method]][[noise]][[instance]]
confusion_matrix_5 <- instancesCM_list[[dataset]][["Fold_5"]][[method]][[noise]][[instance]]
# Store confusion matrices in a list
confusion_matrices <- list(confusion_matrix_1$table, confusion_matrix_2$table, confusion_matrix_3$table, confusion_matrix_4$table, confusion_matrix_5$table)
# Initialize a sum matrix with zeros
sum_matrix <- matrix(0, nrow = nrow(confusion_matrix_1$table), ncol = ncol(confusion_matrix_1$table))
# Add confusion matrices to the sum matrix element-wise
for (i in 1:length(confusion_matrices)) {
sum_matrix <- sum_matrix + confusion_matrices[[i]]
}
print("Total Sum Confusion Matrix:")
print(sum_matrix)
print(paste("Checking matrices for:", dataset, method, noise, instance))
print(paste0("CM1: ", !is.null(confusion_matrix_1) , " has table ", !is.null(confusion_matrix_1) && !is.null(confusion_matrix_1$table)))
# Calculate the average by dividing by the total number of confusion matrices
average_matrix <- sum_matrix / length(confusion_matrices)
if(noise == "noise_0") {
kappa <- 1
accuracy <- 1
} else if(instance == "0") {
kappa <- 1
accuracy <- 1
} else {
# Obtain average Accuracy and Kappa
# Compute the observed and expected agreements
total_obs <- sum(average_matrix)
P_o <- sum(diag(average_matrix)) / total_obs
P_e <- sum(rowSums(average_matrix) * colSums(average_matrix)) / (total_obs^2)
# Calculate Cohen's Kappa
kappa <- (P_o - P_e) / (1 - P_e)
# Calculate accuracy from a confusion matrix
accuracy <- sum(diag(average_matrix)) / sum(average_matrix)
if(kappa == "NaN") {kappa <- 1}
}
# Print relevant data
print(paste0("Technique: ", method))
print(paste0("Noise level: ", noise))
print(paste0("Instance percentage: ", instance))
# Print the average confusion matrix
print("Average Confusion Matrix:")
print(average_matrix)
# Print the average Cohen's Kappa
print("Average Cohen's Kappa:")
print(kappa)
# Print the accuracy
print("Accuracy:")
print(accuracy)
# Store elements in list
item_list[[1]] <- average_matrix
item_list[[2]] <- kappa
item_list[[3]] <- accuracy
names(item_list) <-  c("confusion_matrix", "kappa", "accuracy")
percentagesCM_list[[p_counter]] <- item_list
p_counter = p_counter + 1
print("---")
}
names(percentagesCM_list) <- instances_names
print(instances_names)
noiseCM_list[[n_counter]] <- percentagesCM_list
n_counter = n_counter + 1
}
names(noiseCM_list) <- noise_names
methodCM_list[[m_counter]] <- noiseCM_list
m_counter = m_counter + 1
}
names(methodCM_list) <- method_names
confMatrices_list[[cm_counter]] <- methodCM_list
cm_counter = cm_counter + 1
filename1 = paste0("results/matrices_results/matrices_popular/", dataset, "_confusion_matrix_popular.rds")
saveRDS(confMatrices_list, file = filename1)
print(paste0("Recorded matrices, kappa and accuracy for dataset: ", dataset))
print("----------------")
}
# /usr/bin/env/Rscript
setwd("~/rscripts/code")
# Packages that need to be loaded
#pacman::p_load(caret, iml, xtable, ggpubr, citation, dplyr, data.table)
# Set the seed to make the experiment reproducible
set.seed(1)
# Load important data
## Load files
args <- commandArgs(trailingOnly = TRUE)
datasets <- args
datasets <- readRDS("files/datasets.rds")
fold_names <- readRDS("files/folds.rds")
#method_names <- readRDS("files/method_names.rds")
method_names = c("C5.0", "ctree", "fda", "gbm", "gcvEarth", "JRip", "lvq", "mlpML", "multinom", "naive_bayes", "PART", "rbfDDA", "rda", "rf", "rpart", "simpls", "svmRadial", "rfRules", "knn", "bayesglm")
noise_names <- readRDS("files/noise_names.rds")
instances_names = readRDS("files/instances_names.rds")
#instances_names = append(readRDS("files/instances_names.rds"), c("25", "75"))
## Load results
mia_df <- readRDS("results/mia_df.rds")
noiseMIA_list <- readRDS("results/noise_list.rds")
instancesCM_list = readRDS("results/instancesCM_list_popular.rds")
confMatrices_list <- list()
cm_counter = 1
for(dataset in datasets) {
# Load dataset
#dataset <- "vowel"
filename = paste0("datasets/", dataset, ".rds")
df <- readRDS(filename)
methodCM_list <- list()
m_counter = 1
for(method in method_names){
#method = "C5.0"
noiseCM_list <- list()
n_counter = 1
for(noise in noise_names) {
#noise = "noise_20"
percentagesCM_list <- list()
p_counter = 1
for(instance in instances_names){
#instance = 0
item_list <- list()
# Obtain each confusion matrix from all five folds
confusion_matrix_1 <- instancesCM_list[[dataset]][["Fold_1"]][[method]][[noise]][[instance]]
confusion_matrix_2 <- instancesCM_list[[dataset]][["Fold_2"]][[method]][[noise]][[instance]]
confusion_matrix_3 <- instancesCM_list[[dataset]][["Fold_3"]][[method]][[noise]][[instance]]
confusion_matrix_4 <- instancesCM_list[[dataset]][["Fold_4"]][[method]][[noise]][[instance]]
confusion_matrix_5 <- instancesCM_list[[dataset]][["Fold_5"]][[method]][[noise]][[instance]]
# Store confusion matrices in a list
confusion_matrices <- list(confusion_matrix_1$table, confusion_matrix_2$table, confusion_matrix_3$table, confusion_matrix_4$table, confusion_matrix_5$table)
# Initialize a sum matrix with zeros
sum_matrix <- matrix(0, nrow = nrow(confusion_matrix_1$table), ncol = ncol(confusion_matrix_1$table))
# Add confusion matrices to the sum matrix element-wise
for (i in 1:length(confusion_matrices)) {
sum_matrix <- sum_matrix + confusion_matrices[[i]]
}
print("Total Sum Confusion Matrix:")
print(sum_matrix)
print(paste("Checking matrices for:", dataset, method, noise, instance))
print(paste0("CM1: ", !is.null(confusion_matrix_1) , " has table ", !is.null(confusion_matrix_1) && !is.null(confusion_matrix_1$table)))
# Calculate the average by dividing by the total number of confusion matrices
average_matrix <- sum_matrix / length(confusion_matrices)
if(noise == "noise_0") {
kappa <- 1
accuracy <- 1
} else if(instance == "0") {
kappa <- 1
accuracy <- 1
} else {
# Obtain average Accuracy and Kappa
# Compute the observed and expected agreements
total_obs <- sum(average_matrix)
P_o <- sum(diag(average_matrix)) / total_obs
P_e <- sum(rowSums(average_matrix) * colSums(average_matrix)) / (total_obs^2)
# Calculate Cohen's Kappa
kappa <- (P_o - P_e) / (1 - P_e)
# Calculate accuracy from a confusion matrix
accuracy <- sum(diag(average_matrix)) / sum(average_matrix)
if(kappa == "NaN") {kappa <- 1}
}
# Print relevant data
print(paste0("Technique: ", method))
print(paste0("Noise level: ", noise))
print(paste0("Instance percentage: ", instance))
# Print the average confusion matrix
print("Average Confusion Matrix:")
print(average_matrix)
# Print the average Cohen's Kappa
print("Average Cohen's Kappa:")
print(kappa)
# Print the accuracy
print("Accuracy:")
print(accuracy)
# Store elements in list
item_list[[1]] <- average_matrix
item_list[[2]] <- kappa
item_list[[3]] <- accuracy
names(item_list) <-  c("confusion_matrix", "kappa", "accuracy")
percentagesCM_list[[p_counter]] <- item_list
p_counter = p_counter + 1
print("---")
}
names(percentagesCM_list) <- instances_names
print(instances_names)
noiseCM_list[[n_counter]] <- percentagesCM_list
n_counter = n_counter + 1
}
names(noiseCM_list) <- noise_names
methodCM_list[[m_counter]] <- noiseCM_list
m_counter = m_counter + 1
}
names(methodCM_list) <- method_names
confMatrices_list[[cm_counter]] <- methodCM_list
cm_counter = cm_counter + 1
filename1 = paste0("results/matrices_results/matrices_popular/", dataset, "_confusion_matrix_popular.rds")
saveRDS(confMatrices_list, file = filename1)
print(paste0("Recorded matrices, kappa and accuracy for dataset: ", dataset))
print("----------------")
}
setwd("~/rscripts/code")
# Load files
centroids_1 <- readRDS("files/clustering/centroids.rds")
characteristics_1 <- readRDS("files/clustering/characteristics.rds")
cl_scaled_1 <- readRDS("files/clustering/cl_scaled.rds")
dmatrix_1 <- readRDS("files/clustering/dmatrix.rds")
groups_1 <- readRDS("files/clustering/groups.rds")
kcca_1 <- readRDS("files/clustering/kcca.rds")
kmeans_1 <- readRDS("files/clustering/kmeans.rds")
plots_1 <- readRDS("files/clustering/plots.rds")
test_data_1 <- readRDS("files/clustering/test_data.rds")
test_set_1 <- readRDS("files/clustering/test_set.rds")
train_dmatrix_1 <- readRDS("files/clustering/train_dmatrix.rds")
train_set_1 <- readRDS("files/clustering/train_set.rds")
centroids_2 <- readRDS("results/clustering/centroids.rds")
characteristics_2 <- readRDS("results/clustering/characteristics.rds")
cl_scaled_2 <- readRDS("results/clustering/cl_scaled.rds")
dmatrix_2 <- readRDS("results/clustering/dmatrix.rds")
groups_2 <- readRDS("results/clustering/groups.rds")
kcca_2 <- readRDS("results/clustering/kcca.rds")
plots_2 <- readRDS("results/clustering/plots.rds")
test_data_2 <- readRDS("results/clustering/test_data.rds")
test_set_2 <- readRDS("results/clustering/test_set.rds")
train_dmatrix_2 <- readRDS("results/clustering/train_dmatrix.rds")
train_set_2 <- readRDS("results/clustering/train_set.rds")
print(centroids_1)
print(centroids_2)
print(characteristics_1)
print(characteristics_2)
knitr::opts_chunk$set(echo = TRUE)
# Packages that need to be loaded
pacman::p_load(ggplot2, tidyverse , tidyr, factoextra, proxy, dominanceanalysis, clustertend, MASS, smacof, vegan, cluster, flexclust)
library(ggplot2)
library(dplyr)
library(reshape2)
library(patchwork)
# Load files
datasets <- c("analcatdata_authorship", "badges2", "banknote", "blood-transfusion-service-center", "breast-w", "cardiotocography", "climate-model-simulation-crashes", "cmc", "credit-g", "diabetes", "eucalyptus", "iris", "kc1", "liver-disorders", "mfeat-karhunen", "mfeat-zernike", "ozone-level-8hr", "pc4", "phoneme", "qsar-biodeg", "tic-tac-toe", "vowel", "waveform-5000", "wdbc", "wilt")
method_names = c("C5.0", "ctree", "fda", "gbm", "gcvEarth", "JRip", "lvq", "mlpML", "multinom", "naive_bayes", "PART", "rbfDDA", "rda", "rf", "rpart", "simpls", "svmLinear", "svmRadial", "rfRules", "knn", "bayesglm")
noise_level = c(0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)
noise_names = c("noise_0", "noise_5", "noise_10", "noise_20", "noise_30", "noise_40", "noise_50", "noise_60", "noise_70", "noise_80", "noise_90", "noise_100")
instances_names <- c("0", "10", "20", "30", "40", "50", "60", "70", "80", "90", "100") # Set names for percentages of instances to be altered
quartiles_names = c("0", "25", "50", "75", "100")
train_set <- readRDS("files/clustering/train_set.rds")
train_set <- readRDS("~/results/files/clustering/train_set.rds")
train_set <- readRDS("../files/clustering/train_set.rds")
# Load files
datasets <- c("analcatdata_authorship", "badges2", "banknote", "blood-transfusion-service-center", "breast-w", "cardiotocography", "climate-model-simulation-crashes", "cmc", "credit-g", "diabetes", "eucalyptus", "iris", "kc1", "liver-disorders", "mfeat-karhunen", "mfeat-zernike", "ozone-level-8hr", "pc4", "phoneme", "qsar-biodeg", "tic-tac-toe", "vowel", "waveform-5000", "wdbc", "wilt")
method_names = c("C5.0", "ctree", "fda", "gbm", "gcvEarth", "JRip", "lvq", "mlpML", "multinom", "naive_bayes", "PART", "rbfDDA", "rda", "rf", "rpart", "simpls", "svmLinear", "svmRadial", "rfRules", "knn", "bayesglm")
noise_level = c(0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)
noise_names = c("noise_0", "noise_5", "noise_10", "noise_20", "noise_30", "noise_40", "noise_50", "noise_60", "noise_70", "noise_80", "noise_90", "noise_100")
instances_names <- c("0", "10", "20", "30", "40", "50", "60", "70", "80", "90", "100") # Set names for percentages of instances to be altered
quartiles_names = c("0", "25", "50", "75", "100")
train_set <- readRDS("../files/clustering/train_set.rds")
test_set <- readRDS("../files/clustering/test_set.rds")
kmeans <- readRDS("../files/clustering/kmeans.rds")
centroids_4 <- readRDS("../files/clustering/centroids.rds")
kcca <- readRDS("../files/clustering/kcca.rds")
# Visualize the clusters
plot(fviz_cluster(kmeans, data = train_set[,-1],
geom = "point", ellipse.type = "convex",
ggtheme = theme_minimal()) + coord_fixed(ratio = 1))
quartiles_df <- readRDS(file = "results/results_plot_q.rds")
quartiles_df <- readRDS(file = "../results/results_plot_q.rds")
# Create a new column to control the order of datasets
quartiles_df$dataset_order <- factor(quartiles_df$dataset_name, levels = datasets)
# Create a new column to control the order of methods
quartiles_df$method_order <- factor(quartiles_df$technique, levels = method_names)
# Create plot
p <- ggplot(quartiles_df, aes(x = percentage, y = kappa_loss, color = factor(noise))) +
geom_point() +
geom_line(aes(group = factor(noise))) +
labs(x = "Instances", y = "Kappa", color = "Noise") +
theme_bw() +
scale_y_continuous(limits = c(0.0, 1), breaks = seq(0, 1, by = 0.1)) +
facet_grid(method_order ~ dataset_order, scales = "free")
# Print plot
print(p)
# Create plots for visualizing kappa loss per cluster and quartile
create_cluster_quartile_plots <- function(quartiles_df, kmeans_obj, datasets, method_names,
save_path = NULL, combined = FALSE) {
# Step 1: Create a mapping from dataset name to cluster
dataset_cluster_map <- data.frame(
dataset_name = datasets,
cluster = kmeans_obj$cluster
)
# Step 2: Join cluster information to the main dataframe
data_with_clusters <- merge(quartiles_df, dataset_cluster_map, by = "dataset_name")
# Step 3: Get unique clusters and quartiles
clusters <- sort(unique(data_with_clusters$cluster))
quartiles <- sort(unique(data_with_clusters$percentage))
# Step 4: Convert noise column to numeric for proper ordering
# Assuming noise column is in format "noise_X" where X is the percentage
data_with_clusters <- data_with_clusters %>%
mutate(noise_numeric = as.numeric(str_replace(str_replace(noise, "noise_", ""), "00$", "0")) / 100)
# Step 5: Create plots
plot_list <- list()
for (cluster_id in clusters) {
for (q in quartiles) {
# Filter data for this cluster and quartile
plot_data <- data_with_clusters %>%
filter(cluster == cluster_id, percentage == q)
# Create plot
p <- ggplot(plot_data, aes(x = noise_numeric, y = kappa_loss, color = technique, group = technique)) +
geom_line() +
geom_point(size = 1.5) +
scale_x_continuous(
name = "Noise Level",
breaks = seq(0, 1, by = 0.1),
labels = scales::percent_format()
) +
scale_y_continuous(
name = "Kappa Loss",
limits = c(0, 1),
breaks = seq(0, 1, by = 0.1)
) +
scale_color_viridis_d(name = "Method") +  # Better color palette for many methods
ggtitle(paste("Cluster", cluster_id, "- Quartile", q, "%")) +
theme_minimal() +
theme(
legend.position = "right",
legend.text = element_text(size = 8),
panel.grid.minor = element_blank(),
plot.title = element_text(size = 12, face = "bold"),
axis.title = element_text(size = 10),
axis.text = element_text(size = 8)
)
# Store in list
plot_name <- paste("Cluster", cluster_id, "Quartile", q)
plot_list[[plot_name]] <- p
# Save individual plot if path provided
if (!is.null(save_path)) {
file_name <- file.path(save_path, paste0("cluster_", cluster_id, "_quartile_", q, ".pdf"))
ggsave(file_name, p, width = 10, height = 7)
}
}
}
# If combined is TRUE, create a combined plot arrangement
if (combined) {
combined_plots <- list()
for (cluster_id in clusters) {
# Get all plots for this cluster
cluster_plots <- plot_list[grep(paste0("Cluster ", cluster_id), names(plot_list))]
# Combine using patchwork
combined_plot <- wrap_plots(cluster_plots, ncol = length(quartiles))
combined_plots[[paste0("Cluster_", cluster_id)]] <- combined_plot
# Save if path provided
if (!is.null(save_path)) {
file_name <- file.path(save_path, paste0("combined_cluster_", cluster_id, ".pdf"))
ggsave(file_name, combined_plot, width = 18, height = 10)
}
}
return(combined_plots)
}
return(plot_list)
}
# Alternative approach using facets
create_faceted_plots <- function(quartiles_df, kmeans_obj, datasets,
save_path = NULL) {
# Create dataset to cluster mapping
dataset_cluster_map <- data.frame(
dataset_name = datasets,
cluster = kmeans_obj$cluster
)
# Join cluster information
data_with_clusters <- merge(quartiles_df, dataset_cluster_map, by = "dataset_name")
# Convert noise to numeric
data_with_clusters <- data_with_clusters %>%
mutate(noise_numeric = as.numeric(str_replace(str_replace(noise, "noise_", ""), "00$", "0")) / 100,
percentage_factor = factor(percentage, levels = c("0", "25", "50", "75", "100")),
cluster_factor = factor(cluster))
# Create faceted plot - one facet grid per cluster
plots <- list()
for (cluster_id in unique(data_with_clusters$cluster)) {
cluster_data <- data_with_clusters %>% filter(cluster == cluster_id)
p <- ggplot(cluster_data, aes(x = noise_numeric, y = kappa_loss, color = technique, group = technique)) +
geom_line() +
geom_point() +
scale_x_continuous(
name = "Noise Level",
breaks = seq(0, 1, by = 0.2),
labels = scales::percent_format()
) +
scale_y_continuous(
name = "Kappa Loss",
limits = c(0, 1),
breaks = seq(0, 1, by = 0.2)
) +
facet_wrap(~ percentage_factor, nrow = 1, labeller = labeller(percentage_factor = function(x) paste0("Quartile ", x, "%"))) +
theme_minimal() +
theme(
legend.position = "right",
strip.text = element_text(size = 10, face = "bold"),
strip.background = element_rect(fill = "lightgrey", color = NA),
panel.spacing = unit(0.5, "lines"),
legend.text = element_text(size = 8)
) +
ggtitle(paste("Cluster", cluster_id)) +
scale_color_viridis_d(name = "Method")
plots[[paste0("Cluster_", cluster_id)]] <- p
if (!is.null(save_path)) {
file_name <- file.path(save_path, paste0("facet_cluster_", cluster_id, ".pdf"))
ggsave(file_name, p, width = 15, height = 7)
}
}
return(plots)
}
setwd("~/github/BDMA-2025")
